# -*- coding: utf-8 -*-
"""mais_final_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14B1qtIYTTGVUGuepp2qJ4kdVXlDnrJGS

Load dataset from kaggle and unzip the data.
*-> Upload kaggle.json file before running*
"""

!mkdir ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 /root/.kaggle/kaggle.json

!kaggle datasets download -d nathalieredick/rocks-and-fossils
!unzip rocks-and-fossils.zip

import os
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import cv2
import PIL
import glob
import re
import math

dataset_path = '/content/datasets3 copy'
classes_path = '/content/datasets3 copy/classes.csv'

"""Load images in color and display some sample images."""

# read in images
image_paths = list(glob.glob(f'{dataset_path}/X_train/*.jpeg'))
X_train = np.stack([cv2.imread(str(x), cv2.IMREAD_COLOR) 
                   for x in image_paths])

image_paths = list(glob.glob(f'{dataset_path}/X_test/*.jpeg'))
X_test = np.stack([cv2.imread(str(x), cv2.IMREAD_COLOR) 
                   for x in image_paths])

# display samples 
img_size = X_train[0].shape
for i in range(5):
  print(X_train[i].shape)
  display(PIL.Image.fromarray(X_train[i]))

"""Remove unwanted noise using Gaussian blur."""

def remove_noise(dataset):
  no_noise = []
  for img in dataset:
      blur = cv2.GaussianBlur(img, (5, 5), 0)
      no_noise.append(blur)
  return no_noise

'''X_train_og = X_train
X_train = remove_noise(X_train)
X_test_og = X_test
X_test = remove_noise(X_test)'''

#display('Original', PIL.Image.fromarray(X_train_og[0]), 'Blurred', PIL.Image.fromarray(X_train[0]))

#get the number of classes 
num_classes = sum(1 for row in open(classes_path, 'r'))
print('Number of classes: ', num_classes)

"""Functions to get labels of data from image file name and convert them to integers"""

def get_labels(X):
  y = []
  for item in os.listdir(f'{dataset_path}/{X}/'):
    name = re.findall('^[a-z]+', item)
    y.append(*name)
  return y

y_train = get_labels('X_train')
y_test = get_labels('X_test')
print('y_train: ', y_train, '\ny_test: ', y_test)

def classes_to_nums():
  classes = []
  names = []
  for i, row in enumerate(open(classes_path, 'r')):
    name = re.findall('^[a-z]+', row)
    if name not in names:
      name.append(name)
      classes.append([i, *re.findall('^[a-z]+', row)])
  return classes

def numerical_labels(classes, labels):
  result = []
  for l in labels:
    for c in classes:
      if l == c[1]:
        result.append(c[0])
  return result

# convert image data to numpy arrays 
X_train = np.array(X_train)
X_test = np.array(X_test)

# get encoded labels of each set
numerical_classes = classes_to_nums()
y_train = numerical_labels(numerical_classes, y_train)
y_test = numerical_labels(numerical_classes, y_test)

"""Instantiate, train, and evaluate the CNN."""

# imports for the CNN
from __future__ import print_function
import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.utils import to_categorical
from keras.layers import Conv2D, MaxPooling2D, BatchNormalization
from keras.callbacks import EarlyStopping, ModelCheckpoint

# one hot encode data labels for train and test data
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

# settings for CNN model
batch_size = 25
epochs = 20
save_dir = os.path.join(os.getcwd(), 'saved_models')
model_name = 'rocks_and_fossils.h5'

# print the data, split between train and test sets:
print('X_train shape:', X_train.shape, '\t', X_train.shape[0], 'train samples')
print('X_test shape:', X_test.shape,'\t', X_test.shape[0], 'test samples')

model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3),
                 activation='relu',
                 input_shape=X_train.shape[1:]))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes, activation='softmax'))

callbacks = [EarlyStopping(monitor='val_loss', patience=4),
             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]

model.compile(loss='categorical_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy', 'mse'])

model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, callbacks=callbacks)

# save model and weights
if not os.path.isdir(save_dir):
    os.makedirs(save_dir)
model_path = os.path.join(save_dir, model_name)
model.save(model_path)
print(f'Saved trained model at {model_path}')

# score trained model
scores = model.evaluate(X_test, y_test, verbose=1)
print('Test loss:', scores[0])
print('Test accuracy:', scores[1])

"""Sources:

[Building a CNN (1)](https://towardsdatascience.com/building-a-convolutional-neural-network-cnn-in-keras-329fbbadc5f5)

[Building a CNN (2)](https://towardsdatascience.com/build-your-own-convolution-neural-network-in-5-mins-4217c2cf964f)

[Early Stopping](https://chrisalbon.com/deep_learning/keras/neural_network_early_stopping/)
"""